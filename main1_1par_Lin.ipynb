{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pytse_client as tse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# tickers = tse.download(symbols=\"all\", write_to_csv=True)\n",
    "\n",
    "Device = (\"cuda\"\n",
    "          if torch.cuda.is_available()\n",
    "          else \"cpu\"\n",
    ")\n",
    "print(f\"Using {Device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym=\"آباد\"\n",
    "ticker=tse.Ticker(sym)\n",
    "length=ticker.history.shape[0]\n",
    "train_length = int(np.floor(length*0.9))\n",
    "test_length = length - train_length\n",
    "days_history=100\n",
    "days_predict=30\n",
    "\n",
    "# Now we extract some portion of the first sequences of data for train\n",
    "X_train=np.zeros([train_length - days_history - days_predict - 1 , days_history])\n",
    "Y_train=np.zeros([train_length - days_history - days_predict - 1 , days_predict])\n",
    "for i in range(train_length - days_history - days_predict - 1):\n",
    "    m_pre = ticker.history.loc[i : i + days_history - 1].to_numpy()\n",
    "    m_next = ticker.history.loc[i+1 : i + days_history].to_numpy()\n",
    "    X_train[i,:] = (m_next[:,9]-m_pre[:,9])/m_pre[:,9] * 100\n",
    "    m_pre = ticker.history.loc[i + days_history : i + days_history - 1 + days_predict].to_numpy()\n",
    "    m_next = ticker.history.loc[i + days_history + 1 : i + days_history + days_predict].to_numpy()\n",
    "    Y_train[i,:] = (m_next[:,9]-m_pre[:,9])/m_pre[:,9] * 100\n",
    "\n",
    "# Now we extract some portion of the last sequences of data for test\n",
    "X_test=np.zeros([test_length - days_predict - 1 , days_history])\n",
    "Y_test=np.zeros([test_length - days_predict - 1 , days_predict])\n",
    "for i in range(test_length - days_predict - 1):\n",
    "    j = i + train_length - days_history\n",
    "    m_pre = ticker.history.loc[j : j + days_history - 1].to_numpy()\n",
    "    m_next = ticker.history.loc[j+1 : j + days_history].to_numpy()\n",
    "    X_test[i,:] = (m_next[:,9]-m_pre[:,9])/m_pre[:,9] * 100\n",
    "    m_pre = ticker.history.loc[j + days_history : j + days_history - 1 + days_predict].to_numpy()\n",
    "    m_next = ticker.history.loc[j + days_history + 1 : j + days_history + days_predict].to_numpy()\n",
    "    Y_test[i,:] = (m_next[:,9]-m_pre[:,9])/m_pre[:,9] * 100\n",
    "\n",
    "train_arr = np.arange(train_length - days_history - days_predict - 1)\n",
    "train_idx = np.random.permutation(train_arr)\n",
    "X_train = X_train[train_idx]\n",
    "Y_train = Y_train[train_idx]\n",
    "\n",
    "test_arr = np.arange(test_length - days_predict - 1)\n",
    "test_idx = np.random.permutation(test_arr)\n",
    "X_test = X_test[test_idx]\n",
    "Y_test = Y_test[test_idx]\n",
    "X_train = torch.from_numpy(X_train).to(device=Device,dtype=torch.float32)\n",
    "Y_train = torch.from_numpy(Y_train).to(device=Device,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).to(device=Device,dtype=torch.float32)\n",
    "Y_test = torch.from_numpy(Y_test).to(device=Device,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        n=64\n",
    "        super().__init__()\n",
    "        self.Lin1 = nn.Linear(input_size,2*n)\n",
    "        self.Lin2 = nn.Linear(2*n,4*n)\n",
    "        self.Lin3 = nn.Linear(4*n,2*n)\n",
    "        self.Lin4 = nn.Linear(2*n,n)\n",
    "        self.Lin5 = nn.Linear(n,output_size)\n",
    "        self.activation1 = nn.LeakyReLU(negative_slope=0.1)\n",
    "        self.activation2 = nn.Tanh()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Lin1(x)\n",
    "            \n",
    "        x = self.activation2(x)\n",
    "        x = self.drop(x) ###\n",
    "        x = self.Lin2(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.drop(x) ###\n",
    "        x = self.Lin3(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.drop(x) ###\n",
    "            \n",
    "\n",
    "        x = self.Lin4(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.drop(x) ###\n",
    "        x = self.Lin5(x)\n",
    "        x = self.drop(x) ###\n",
    "        return x\n",
    "gama=0.99\n",
    "W_gama=torch.diag(torch.tensor([gama**(1*i) for i in range(days_predict)],device=Device))\n",
    "model=Model(days_history,days_predict).to(device=Device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "LossFn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: train loss is 9.869823455810547 & test loss is 39.126277923583984 & accuracy is %nan\n",
      "1: train loss is 9.863832473754883 & test loss is 39.121742248535156 & accuracy is %nan\n",
      "2: train loss is 9.874431610107422 & test loss is 39.1171989440918 & accuracy is %nan\n",
      "3: train loss is 9.870155334472656 & test loss is 39.11210632324219 & accuracy is %nan\n",
      "4: train loss is 9.870301246643066 & test loss is 39.1139030456543 & accuracy is %nan\n",
      "5: train loss is 9.870537757873535 & test loss is 39.124778747558594 & accuracy is %nan\n",
      "6: train loss is 9.870307922363281 & test loss is 39.123680114746094 & accuracy is %nan\n",
      "7: train loss is 9.878641128540039 & test loss is 39.118988037109375 & accuracy is %nan\n",
      "8: train loss is 9.88473892211914 & test loss is 39.14191818237305 & accuracy is %nan\n",
      "9: train loss is 9.89375114440918 & test loss is 39.12363052368164 & accuracy is %nan\n",
      "10: train loss is 9.875864028930664 & test loss is 39.12308120727539 & accuracy is %nan\n",
      "11: train loss is 9.862142562866211 & test loss is 39.12740707397461 & accuracy is %nan\n",
      "12: train loss is 9.885417938232422 & test loss is 39.10759353637695 & accuracy is %nan\n",
      "13: train loss is 9.873205184936523 & test loss is 39.11020278930664 & accuracy is %nan\n",
      "14: train loss is 9.891371726989746 & test loss is 39.11095428466797 & accuracy is %nan\n",
      "15: train loss is 9.881119728088379 & test loss is 39.115089416503906 & accuracy is %nan\n",
      "16: train loss is 9.876681327819824 & test loss is 39.115779876708984 & accuracy is %nan\n",
      "17: train loss is 9.873154640197754 & test loss is 39.10926055908203 & accuracy is %nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m Y_train[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size,:]\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m LossFn(y,out)\n\u001b[0;32m     16\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin2(x)\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation1(x)\n\u001b[1;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin3(x)\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation2(x)\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_Losses=[]\n",
    "test_Losses=[]\n",
    "N_train = int(np.floor(X_train.size(0)/batch_size))\n",
    "N_test = int(np.floor(X_test.size(0)/batch_size))\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    train_loss=torch.tensor(0,dtype=torch.float32)\n",
    "    for i in range(N_train):\n",
    "        x = X_train[i*batch_size:(i+1)*batch_size,:]\n",
    "        y = Y_train[i*batch_size:(i+1)*batch_size,:]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss_train = LossFn(y,out)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_train.detach().item()\n",
    "    train_Losses.append( train_loss/N_train )\n",
    "   \n",
    "    model.eval()\n",
    "    test_loss=torch.tensor(0,dtype=torch.float32)\n",
    "    test_acc=torch.tensor(0,dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        for i in range(N_test):\n",
    "            x = X_test[i*batch_size:(i+1)*batch_size,:]\n",
    "            y = Y_test[i*batch_size:(i+1)*batch_size,:].detach()\n",
    "            out = model(x)\n",
    "            loss_test = LossFn(y,out)\n",
    "            test_loss += loss_test.detach().item()\n",
    "            temp=y-out\n",
    "            test_acc += torch.sum(1 - torch.abs(y@W_gama-out@W_gama)/y@W_gama).detach().item()\n",
    "    test_Losses.append( test_loss/N_test )\n",
    "    accuracy = test_acc / (N_test*batch_size*days_predict) *100\n",
    "    print(f'{epoch}: train loss is {train_Losses[epoch]} & test loss is {test_Losses[epoch]} & accuracy is %{accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
