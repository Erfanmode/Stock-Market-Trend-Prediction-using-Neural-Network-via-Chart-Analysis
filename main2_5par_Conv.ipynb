{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import pytse_client as tse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# tickers = tse.download(symbols=\"all\", write_to_csv=True)\n",
    "\n",
    "Device = (\"cuda\"\n",
    "          if torch.cuda.is_available()\n",
    "          else \"cpu\"\n",
    ")\n",
    "print(f\"Using {Device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym=\"آپ\"\n",
    "ticker=tse.Ticker(sym)\n",
    "length=ticker.history.shape[0]\n",
    "train_length = int(np.floor(length*0.90))\n",
    "test_length = length - train_length\n",
    "days_history=50\n",
    "days_predict=8\n",
    "\n",
    "# Now we extract some portion of the first sequences of data for train\n",
    "X_train=np.zeros([train_length - days_history - days_predict , days_history,5])\n",
    "Y_train=np.zeros([train_length - days_history - days_predict , days_predict])\n",
    "for i in range(train_length - days_history - days_predict):\n",
    "    m = ticker.history.loc[i : i + days_history - 1].to_numpy()\n",
    "    X_train[i,:,:] = np.array([(m[:,9]-m[:,8])/m[:,8] * 100 , m[:,7]/1e3 , m[:,6]/1e6 , m[:,2] , m[:,3]]).T\n",
    "    m = ticker.history.loc[i + days_history : i + days_history - 1 + days_predict].to_numpy()\n",
    "    Y_train[i,:] = (m[:,9]-m[:,8])/m[:,8] * 100\n",
    "\n",
    "# Now we extract some portion of the last sequences of data for test\n",
    "X_test=np.zeros([test_length - days_predict , days_history,5])\n",
    "Y_test=np.zeros([test_length - days_predict , days_predict])\n",
    "for i in range(test_length - days_predict):\n",
    "    j = i + train_length - days_history\n",
    "    m = ticker.history.loc[j : j + days_history - 1].to_numpy()\n",
    "    X_test[i,:,:] = np.array([(m[:,9]-m[:,8])/m[:,8] * 100 , m[:,7]/1e3 , m[:,6]/1e6 , m[:,2] , m[:,3]]).T\n",
    "    m = ticker.history.loc[j + days_history : j + days_history - 1 + days_predict].to_numpy()\n",
    "    Y_test[i,:] = (m[:,9]-m[:,8])/m[:,8] * 100\n",
    "\n",
    "train_arr = np.arange(train_length - days_history - days_predict - 1)\n",
    "train_idx = np.random.permutation(train_arr)\n",
    "X_train = X_train[train_idx]\n",
    "Y_train = Y_train[train_idx]\n",
    "\n",
    "test_arr = np.arange(test_length - days_predict - 1)\n",
    "test_idx = np.random.permutation(test_arr)\n",
    "X_test = X_test[test_idx]\n",
    "Y_test = Y_test[test_idx]\n",
    "X_train = torch.from_numpy(X_train).to(device=Device,dtype=torch.float32)\n",
    "Y_train = torch.from_numpy(Y_train).to(device=Device,dtype=torch.float32)\n",
    "\n",
    "X_test = torch.from_numpy(X_test).to(device=Device,dtype=torch.float32)\n",
    "Y_test = torch.from_numpy(Y_test).to(device=Device,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        n=64\n",
    "        super().__init__()\n",
    "        self.Lin1 = nn.Linear(8*input_size,2*n)\n",
    "        self.Lin2 = nn.Linear(2*n,4*n)\n",
    "        self.Lin3 = nn.Linear(4*n,2*n)\n",
    "        self.Lin4 = nn.Linear(2*n,n)\n",
    "        self.Lin5 = nn.Linear(n,output_size)\n",
    "        self.activation1 = nn.LeakyReLU(negative_slope=0.1)\n",
    "        #self.activation2 = nn.Tanh()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(1,8,padding=4,kernel_size=9)\n",
    "        self.conv2 = nn.Conv1d(8,16,padding=4,kernel_size=9)\n",
    "        self.conv3 = nn.Conv1d(16,8,padding=4,kernel_size=9)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        for i in range(1):\n",
    "            x_ = x\n",
    "            x = self.conv2(x)\n",
    "            x = self.activation1(x)\n",
    "            x = self.drop(x) ###\n",
    "            x = self.conv3(x)\n",
    "            x = self.activation1(x)\n",
    "            x = self.drop(x) ###\n",
    "            x = x + x_\n",
    "        \n",
    "        x = self.flat(x)\n",
    "        x = self.Lin1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.Lin2(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.Lin3(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.drop(x) ###\n",
    "        x = self.Lin4(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.drop(x) ###\n",
    "        x = self.Lin5(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.drop(x) ###\n",
    "        return x\n",
    "\n",
    "model=Model(days_history*5,days_predict).to(device=Device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "LossFn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: train loss is 7923.12646484375 & test loss is 10.700337409973145 & accuracy is %-31.782133102416992\n",
      "1: train loss is 166.1512451171875 & test loss is 5.553189277648926 & accuracy is %10.318767547607422\n",
      "2: train loss is 59.54113006591797 & test loss is 4.9153523445129395 & accuracy is %15.433857917785645\n",
      "3: train loss is 42.123809814453125 & test loss is 4.851239204406738 & accuracy is %15.708611488342285\n",
      "4: train loss is 37.238521575927734 & test loss is 4.8975958824157715 & accuracy is %15.503625869750977\n",
      "5: train loss is 28.245040893554688 & test loss is 4.832118988037109 & accuracy is %15.792240142822266\n",
      "6: train loss is 21.303571701049805 & test loss is 4.839232921600342 & accuracy is %15.43858528137207\n",
      "7: train loss is 19.3394718170166 & test loss is 4.821220874786377 & accuracy is %15.816805839538574\n",
      "8: train loss is 17.950437545776367 & test loss is 4.841944694519043 & accuracy is %15.307040214538574\n",
      "9: train loss is 16.951169967651367 & test loss is 4.854093074798584 & accuracy is %15.095189094543457\n",
      "10: train loss is 17.205257415771484 & test loss is 4.839951992034912 & accuracy is %15.345918655395508\n",
      "11: train loss is 15.3504638671875 & test loss is 4.859279632568359 & accuracy is %14.989387512207031\n",
      "12: train loss is 14.664344787597656 & test loss is 4.86267614364624 & accuracy is %14.919718742370605\n",
      "13: train loss is 13.761310577392578 & test loss is 4.859506130218506 & accuracy is %14.966773986816406\n",
      "14: train loss is 14.027702331542969 & test loss is 4.862542152404785 & accuracy is %14.917786598205566\n",
      "15: train loss is 13.475801467895508 & test loss is 4.881040573120117 & accuracy is %14.636611938476562\n",
      "16: train loss is 13.17113208770752 & test loss is 4.866020202636719 & accuracy is %14.869705200195312\n",
      "17: train loss is 12.745747566223145 & test loss is 4.873948097229004 & accuracy is %14.741425514221191\n",
      "18: train loss is 12.18320369720459 & test loss is 4.891441822052002 & accuracy is %14.485308647155762\n",
      "19: train loss is 12.19583797454834 & test loss is 4.895475387573242 & accuracy is %14.417186737060547\n",
      "20: train loss is 12.053003311157227 & test loss is 4.895403861999512 & accuracy is %14.417253494262695\n",
      "21: train loss is 11.58757495880127 & test loss is 4.895071029663086 & accuracy is %14.413701057434082\n",
      "22: train loss is 11.787445068359375 & test loss is 4.900146961212158 & accuracy is %14.346960067749023\n",
      "23: train loss is 11.3165922164917 & test loss is 4.8972320556640625 & accuracy is %14.387011528015137\n",
      "24: train loss is 11.540572166442871 & test loss is 4.882447242736816 & accuracy is %14.61009693145752\n",
      "25: train loss is 11.050345420837402 & test loss is 4.8932671546936035 & accuracy is %14.448836326599121\n",
      "26: train loss is 11.333389282226562 & test loss is 4.905246257781982 & accuracy is %14.270512580871582\n",
      "27: train loss is 10.916155815124512 & test loss is 4.923377990722656 & accuracy is %14.011457443237305\n",
      "28: train loss is 11.069351196289062 & test loss is 4.887772560119629 & accuracy is %14.528460502624512\n",
      "29: train loss is 11.36538028717041 & test loss is 4.915223598480225 & accuracy is %14.125564575195312\n",
      "30: train loss is 10.955106735229492 & test loss is 4.916022777557373 & accuracy is %14.113667488098145\n",
      "31: train loss is 10.752464294433594 & test loss is 4.9166483879089355 & accuracy is %14.10638427734375\n",
      "32: train loss is 10.580177307128906 & test loss is 4.888881683349609 & accuracy is %14.510677337646484\n",
      "33: train loss is 10.805769920349121 & test loss is 4.910210609436035 & accuracy is %14.196391105651855\n",
      "34: train loss is 10.584783554077148 & test loss is 4.9206671714782715 & accuracy is %14.048038482666016\n",
      "35: train loss is 10.586138725280762 & test loss is 4.924197196960449 & accuracy is %14.000489234924316\n",
      "36: train loss is 10.60421085357666 & test loss is 4.9221696853637695 & accuracy is %14.025168418884277\n",
      "37: train loss is 10.532330513000488 & test loss is 4.917141914367676 & accuracy is %14.096853256225586\n",
      "38: train loss is 10.540372848510742 & test loss is 4.9208502769470215 & accuracy is %14.046796798706055\n",
      "39: train loss is 10.626684188842773 & test loss is 4.896210193634033 & accuracy is %14.401809692382812\n",
      "40: train loss is 10.65771770477295 & test loss is 4.924766540527344 & accuracy is %13.988020896911621\n",
      "41: train loss is 10.517620086669922 & test loss is 4.924633026123047 & accuracy is %13.99020004272461\n",
      "42: train loss is 10.585342407226562 & test loss is 4.924007892608643 & accuracy is %14.003639221191406\n",
      "43: train loss is 10.538670539855957 & test loss is 4.918412685394287 & accuracy is %14.081330299377441\n",
      "44: train loss is 10.427755355834961 & test loss is 4.919403553009033 & accuracy is %14.065420150756836\n",
      "45: train loss is 10.425568580627441 & test loss is 4.924412727355957 & accuracy is %13.994134902954102\n",
      "46: train loss is 10.367956161499023 & test loss is 4.931410312652588 & accuracy is %13.898573875427246\n",
      "47: train loss is 10.267971992492676 & test loss is 4.928536415100098 & accuracy is %13.936905860900879\n",
      "48: train loss is 10.30490779876709 & test loss is 4.914002418518066 & accuracy is %14.145262718200684\n",
      "49: train loss is 10.457523345947266 & test loss is 4.925110340118408 & accuracy is %13.985909461975098\n",
      "50: train loss is 11.064372062683105 & test loss is 4.836489200592041 & accuracy is %15.34395980834961\n",
      "51: train loss is 12.41247272491455 & test loss is 4.8771209716796875 & accuracy is %14.697237014770508\n",
      "52: train loss is 10.992599487304688 & test loss is 4.896119594573975 & accuracy is %14.413907051086426\n",
      "53: train loss is 10.765892028808594 & test loss is 4.910524368286133 & accuracy is %14.19478988647461\n",
      "54: train loss is 10.507257461547852 & test loss is 4.90758752822876 & accuracy is %14.237323760986328\n",
      "55: train loss is 10.429422378540039 & test loss is 4.926359176635742 & accuracy is %13.96835708618164\n",
      "56: train loss is 10.534403800964355 & test loss is 4.920363426208496 & accuracy is %14.05647087097168\n",
      "57: train loss is 10.326173782348633 & test loss is 4.922328948974609 & accuracy is %14.029921531677246\n",
      "58: train loss is 10.293977737426758 & test loss is 4.912034034729004 & accuracy is %14.172924041748047\n",
      "59: train loss is 10.259490966796875 & test loss is 4.932460784912109 & accuracy is %13.88565731048584\n",
      "60: train loss is 10.348479270935059 & test loss is 4.898122787475586 & accuracy is %14.37696361541748\n",
      "61: train loss is 10.373941421508789 & test loss is 4.925386428833008 & accuracy is %13.978902816772461\n",
      "62: train loss is 10.295254707336426 & test loss is 4.920761585235596 & accuracy is %14.045056343078613\n",
      "63: train loss is 10.308037757873535 & test loss is 4.9243927001953125 & accuracy is %13.996927261352539\n",
      "64: train loss is 10.168874740600586 & test loss is 4.926492214202881 & accuracy is %13.967665672302246\n",
      "65: train loss is 10.164549827575684 & test loss is 4.926609516143799 & accuracy is %13.96465015411377\n",
      "66: train loss is 10.229812622070312 & test loss is 4.926336765289307 & accuracy is %13.964943885803223\n",
      "67: train loss is 10.171141624450684 & test loss is 4.92460298538208 & accuracy is %13.994243621826172\n",
      "68: train loss is 10.152118682861328 & test loss is 4.924533367156982 & accuracy is %13.991494178771973\n",
      "69: train loss is 10.076586723327637 & test loss is 4.930902004241943 & accuracy is %13.9055757522583\n",
      "70: train loss is 10.183385848999023 & test loss is 4.9248175621032715 & accuracy is %13.988907814025879\n",
      "71: train loss is 10.091753959655762 & test loss is 4.919656753540039 & accuracy is %14.060378074645996\n",
      "72: train loss is 10.160534858703613 & test loss is 4.9356255531311035 & accuracy is %13.843022346496582\n",
      "73: train loss is 10.301519393920898 & test loss is 4.92894172668457 & accuracy is %13.93272876739502\n",
      "74: train loss is 10.27730655670166 & test loss is 4.923266410827637 & accuracy is %14.008056640625\n",
      "75: train loss is 10.107187271118164 & test loss is 4.9295454025268555 & accuracy is %13.923588752746582\n",
      "76: train loss is 10.094371795654297 & test loss is 4.923974990844727 & accuracy is %13.999473571777344\n",
      "77: train loss is 10.169532775878906 & test loss is 4.929717540740967 & accuracy is %13.92369270324707\n",
      "78: train loss is 10.141960144042969 & test loss is 4.941254138946533 & accuracy is %13.767579078674316\n",
      "79: train loss is 10.169657707214355 & test loss is 4.90657377243042 & accuracy is %14.25943374633789\n",
      "80: train loss is 10.155580520629883 & test loss is 4.934696674346924 & accuracy is %13.855993270874023\n",
      "81: train loss is 10.184675216674805 & test loss is 4.9296650886535645 & accuracy is %13.924405097961426\n",
      "82: train loss is 10.201669692993164 & test loss is 4.916463851928711 & accuracy is %14.10508918762207\n",
      "83: train loss is 10.102243423461914 & test loss is 4.936697959899902 & accuracy is %13.828343391418457\n",
      "84: train loss is 10.124305725097656 & test loss is 4.9316253662109375 & accuracy is %13.89753532409668\n",
      "85: train loss is 10.048453330993652 & test loss is 4.9245781898498535 & accuracy is %13.995881080627441\n",
      "86: train loss is 10.055255889892578 & test loss is 4.940181732177734 & accuracy is %13.783669471740723\n",
      "87: train loss is 10.197677612304688 & test loss is 4.914041042327881 & accuracy is %14.138158798217773\n",
      "88: train loss is 10.12759017944336 & test loss is 4.931488037109375 & accuracy is %13.89640998840332\n",
      "89: train loss is 10.116437911987305 & test loss is 4.924777030944824 & accuracy is %13.991376876831055\n",
      "90: train loss is 10.124716758728027 & test loss is 4.933855056762695 & accuracy is %13.867383003234863\n",
      "91: train loss is 10.03993034362793 & test loss is 4.935237407684326 & accuracy is %13.85061264038086\n",
      "92: train loss is 10.06119441986084 & test loss is 4.932435989379883 & accuracy is %13.885223388671875\n",
      "93: train loss is 10.08236026763916 & test loss is 4.9387407302856445 & accuracy is %13.801041603088379\n",
      "94: train loss is 10.053494453430176 & test loss is 4.930172920227051 & accuracy is %13.914358139038086\n",
      "95: train loss is 10.065185546875 & test loss is 4.929224967956543 & accuracy is %13.935809135437012\n",
      "96: train loss is 10.185772895812988 & test loss is 4.893491744995117 & accuracy is %14.438364028930664\n",
      "97: train loss is 10.26237964630127 & test loss is 4.905923366546631 & accuracy is %14.263008117675781\n",
      "98: train loss is 10.159919738769531 & test loss is 4.920533180236816 & accuracy is %14.04904842376709\n",
      "99: train loss is 10.104578018188477 & test loss is 4.931059837341309 & accuracy is %13.902544975280762\n",
      "100: train loss is 10.07309341430664 & test loss is 4.933930397033691 & accuracy is %13.866710662841797\n",
      "101: train loss is 10.058353424072266 & test loss is 4.926131725311279 & accuracy is %13.96986198425293\n",
      "102: train loss is 10.117263793945312 & test loss is 4.933158874511719 & accuracy is %13.875463485717773\n",
      "103: train loss is 10.06334114074707 & test loss is 4.924835681915283 & accuracy is %13.987152099609375\n",
      "104: train loss is 10.225761413574219 & test loss is 4.914931774139404 & accuracy is %14.12399673461914\n",
      "105: train loss is 10.276267051696777 & test loss is 4.924538612365723 & accuracy is %13.998763084411621\n",
      "106: train loss is 10.139019966125488 & test loss is 4.92516565322876 & accuracy is %13.983723640441895\n",
      "107: train loss is 10.050987243652344 & test loss is 4.8998703956604 & accuracy is %14.352943420410156\n",
      "108: train loss is 10.124543190002441 & test loss is 4.929614067077637 & accuracy is %13.925829887390137\n",
      "109: train loss is 10.061774253845215 & test loss is 4.946909427642822 & accuracy is %13.698610305786133\n",
      "110: train loss is 10.035102844238281 & test loss is 4.935060977935791 & accuracy is %13.851311683654785\n",
      "111: train loss is 10.030957221984863 & test loss is 4.943732261657715 & accuracy is %13.738226890563965\n",
      "112: train loss is 10.016531944274902 & test loss is 4.941319465637207 & accuracy is %13.76807975769043\n",
      "113: train loss is 9.922256469726562 & test loss is 4.942587375640869 & accuracy is %13.75069808959961\n",
      "114: train loss is 9.970759391784668 & test loss is 4.942835807800293 & accuracy is %13.750633239746094\n",
      "115: train loss is 9.992660522460938 & test loss is 4.929566383361816 & accuracy is %13.923859596252441\n",
      "116: train loss is 9.981534004211426 & test loss is 4.944408416748047 & accuracy is %13.728520393371582\n",
      "117: train loss is 10.032998085021973 & test loss is 4.93665075302124 & accuracy is %13.830002784729004\n",
      "118: train loss is 10.048893928527832 & test loss is 4.934223175048828 & accuracy is %13.860724449157715\n",
      "119: train loss is 10.07377815246582 & test loss is 4.933752059936523 & accuracy is %13.866816520690918\n",
      "120: train loss is 10.036519050598145 & test loss is 4.925522804260254 & accuracy is %13.977056503295898\n",
      "121: train loss is 10.011223793029785 & test loss is 4.936129570007324 & accuracy is %13.835761070251465\n",
      "122: train loss is 9.975892066955566 & test loss is 4.940557479858398 & accuracy is %13.778249740600586\n",
      "123: train loss is 10.006625175476074 & test loss is 4.938399314880371 & accuracy is %13.80623722076416\n",
      "124: train loss is 10.15735912322998 & test loss is 4.93026065826416 & accuracy is %13.916910171508789\n",
      "125: train loss is 10.08985424041748 & test loss is 4.919051647186279 & accuracy is %14.07334041595459\n",
      "126: train loss is 10.000120162963867 & test loss is 4.934706687927246 & accuracy is %13.857075691223145\n",
      "127: train loss is 10.013772964477539 & test loss is 4.938331604003906 & accuracy is %13.808393478393555\n",
      "128: train loss is 9.988588333129883 & test loss is 4.940518379211426 & accuracy is %13.779650688171387\n",
      "129: train loss is 10.096219062805176 & test loss is 4.919158935546875 & accuracy is %14.063668251037598\n",
      "130: train loss is 10.370725631713867 & test loss is 4.919753551483154 & accuracy is %14.062481880187988\n",
      "131: train loss is 10.184629440307617 & test loss is 4.925936698913574 & accuracy is %13.971296310424805\n",
      "132: train loss is 10.406164169311523 & test loss is 4.917618751525879 & accuracy is %14.090331077575684\n",
      "133: train loss is 10.254133224487305 & test loss is 4.930110931396484 & accuracy is %13.915141105651855\n",
      "134: train loss is 10.04773235321045 & test loss is 4.927165985107422 & accuracy is %13.955327987670898\n",
      "135: train loss is 9.927558898925781 & test loss is 4.932580947875977 & accuracy is %13.88129711151123\n",
      "136: train loss is 9.96352481842041 & test loss is 4.939826965332031 & accuracy is %13.787015914916992\n",
      "137: train loss is 9.924988746643066 & test loss is 4.936971187591553 & accuracy is %13.823333740234375\n",
      "138: train loss is 9.917436599731445 & test loss is 4.941099643707275 & accuracy is %13.770425796508789\n",
      "139: train loss is 9.963440895080566 & test loss is 4.94174861907959 & accuracy is %13.761695861816406\n",
      "140: train loss is 9.964451789855957 & test loss is 4.92556619644165 & accuracy is %13.979445457458496\n",
      "141: train loss is 10.004193305969238 & test loss is 4.936071872711182 & accuracy is %13.836116790771484\n",
      "142: train loss is 9.943584442138672 & test loss is 4.938036918640137 & accuracy is %13.809789657592773\n",
      "143: train loss is 9.95107650756836 & test loss is 4.939977645874023 & accuracy is %13.785741806030273\n",
      "144: train loss is 9.865571975708008 & test loss is 4.943533897399902 & accuracy is %13.741196632385254\n",
      "145: train loss is 9.895099639892578 & test loss is 4.930723190307617 & accuracy is %13.904397010803223\n",
      "146: train loss is 9.941916465759277 & test loss is 4.942510604858398 & accuracy is %13.751873016357422\n",
      "147: train loss is 9.937283515930176 & test loss is 4.943553924560547 & accuracy is %13.738068580627441\n",
      "148: train loss is 9.936552047729492 & test loss is 4.942786693572998 & accuracy is %13.75149917602539\n",
      "149: train loss is 9.890661239624023 & test loss is 4.945521831512451 & accuracy is %13.71422004699707\n",
      "150: train loss is 9.911125183105469 & test loss is 4.942824363708496 & accuracy is %13.748323440551758\n",
      "151: train loss is 9.893420219421387 & test loss is 4.945550918579102 & accuracy is %13.715579986572266\n",
      "152: train loss is 9.907691955566406 & test loss is 4.938823223114014 & accuracy is %13.801583290100098\n",
      "153: train loss is 9.907207489013672 & test loss is 4.941152572631836 & accuracy is %13.772103309631348\n",
      "154: train loss is 9.919673919677734 & test loss is 4.947489261627197 & accuracy is %13.69150447845459\n",
      "155: train loss is 10.186161994934082 & test loss is 4.912089824676514 & accuracy is %14.169112205505371\n",
      "156: train loss is 10.09482192993164 & test loss is 4.925629615783691 & accuracy is %13.977782249450684\n",
      "157: train loss is 9.961352348327637 & test loss is 4.935576915740967 & accuracy is %13.842195510864258\n",
      "158: train loss is 9.909116744995117 & test loss is 4.940275192260742 & accuracy is %13.781097412109375\n",
      "159: train loss is 9.891597747802734 & test loss is 4.941601753234863 & accuracy is %13.766053199768066\n",
      "160: train loss is 9.94416618347168 & test loss is 4.940536022186279 & accuracy is %13.779338836669922\n",
      "161: train loss is 10.03097915649414 & test loss is 4.91722297668457 & accuracy is %14.098278045654297\n",
      "162: train loss is 10.227714538574219 & test loss is 4.90897274017334 & accuracy is %14.216348648071289\n",
      "163: train loss is 10.035987854003906 & test loss is 4.933511257171631 & accuracy is %13.869354248046875\n",
      "164: train loss is 10.022112846374512 & test loss is 4.9312744140625 & accuracy is %13.901424407958984\n",
      "165: train loss is 9.961380958557129 & test loss is 4.934466361999512 & accuracy is %13.857812881469727\n",
      "166: train loss is 9.920958518981934 & test loss is 4.943352699279785 & accuracy is %13.743731498718262\n",
      "167: train loss is 9.86610221862793 & test loss is 4.9423675537109375 & accuracy is %13.755796432495117\n",
      "168: train loss is 9.92379093170166 & test loss is 4.934971332550049 & accuracy is %13.851239204406738\n",
      "169: train loss is 9.875450134277344 & test loss is 4.941344738006592 & accuracy is %13.768378257751465\n",
      "170: train loss is 9.843950271606445 & test loss is 4.931947708129883 & accuracy is %13.892356872558594\n",
      "171: train loss is 10.027403831481934 & test loss is 4.928267002105713 & accuracy is %13.941635131835938\n",
      "172: train loss is 9.96565055847168 & test loss is 4.93312931060791 & accuracy is %13.876160621643066\n",
      "173: train loss is 9.9193115234375 & test loss is 4.927506446838379 & accuracy is %13.955381393432617\n",
      "174: train loss is 9.910675048828125 & test loss is 4.935124397277832 & accuracy is %13.84875774383545\n",
      "175: train loss is 9.931180000305176 & test loss is 4.92870569229126 & accuracy is %13.933721542358398\n",
      "176: train loss is 9.945165634155273 & test loss is 4.912976264953613 & accuracy is %14.154555320739746\n",
      "177: train loss is 9.880607604980469 & test loss is 4.9234089851379395 & accuracy is %14.007184028625488\n",
      "178: train loss is 9.928397178649902 & test loss is 4.9359636306762695 & accuracy is %13.837912559509277\n",
      "179: train loss is 9.942278861999512 & test loss is 4.935539245605469 & accuracy is %13.844797134399414\n",
      "180: train loss is 9.91750431060791 & test loss is 4.938497543334961 & accuracy is %13.805476188659668\n",
      "181: train loss is 9.903705596923828 & test loss is 4.942517280578613 & accuracy is %13.753965377807617\n",
      "182: train loss is 9.924365043640137 & test loss is 4.938287258148193 & accuracy is %13.807222366333008\n",
      "183: train loss is 9.91073226928711 & test loss is 4.936372756958008 & accuracy is %13.832283020019531\n",
      "184: train loss is 9.910593032836914 & test loss is 4.936074256896973 & accuracy is %13.837562561035156\n",
      "185: train loss is 9.918224334716797 & test loss is 4.938940048217773 & accuracy is %13.800793647766113\n",
      "186: train loss is 9.87148380279541 & test loss is 4.943005561828613 & accuracy is %13.745336532592773\n",
      "187: train loss is 9.858160972595215 & test loss is 4.9231953620910645 & accuracy is %14.013765335083008\n",
      "188: train loss is 9.865042686462402 & test loss is 4.936666965484619 & accuracy is %13.82746410369873\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m Y_train[i\u001b[38;5;241m*\u001b[39mbatch_size:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size,:]\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m LossFn(y,out)\n\u001b[0;32m     16\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\IT  CITY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation1(x)\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x) \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat(x)\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLin1(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 40\n",
    "train_Losses=[]\n",
    "test_Losses=[]\n",
    "N_train = int(np.floor(X_train.size(0)/batch_size))\n",
    "N_test = int(np.floor(X_test.size(0)/batch_size))\n",
    "\n",
    "for epoch in range(300):\n",
    "    model.train()\n",
    "    train_loss=torch.tensor(0,dtype=torch.float32)\n",
    "    for i in range(N_train):\n",
    "        x = torch.flatten(X_train[i*batch_size:(i+1)*batch_size,:,:],start_dim=1).unsqueeze(dim=1)\n",
    "        y = Y_train[i*batch_size:(i+1)*batch_size,:]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss_train = LossFn(y,out)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss_train.detach().item()\n",
    "    train_Losses.append( train_loss/N_train )\n",
    "   \n",
    "    model.eval()\n",
    "    test_loss=torch.tensor(0,dtype=torch.float32)\n",
    "    test_acc=torch.tensor(0,dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        for i in range(N_test):\n",
    "            x = torch.flatten(X_test[i*batch_size:(i+1)*batch_size,:,:],start_dim=1).unsqueeze(dim=1)\n",
    "            y = Y_test[i*batch_size:(i+1)*batch_size,:].detach()\n",
    "            out = model(x)\n",
    "            loss_test = LossFn(y,out)\n",
    "            test_loss += loss_test.detach().item()\n",
    "            temp=y-out\n",
    "            test_acc += torch.sum(1 - torch.abs(y-out)/2).detach().item()\n",
    "    test_Losses.append( test_loss/N_test )\n",
    "    accuracy = test_acc / (N_test*batch_size*days_predict) *100\n",
    "    print(f'{epoch}: train loss is {train_Losses[epoch]} & test loss is {test_Losses[epoch]} & accuracy is %{accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
